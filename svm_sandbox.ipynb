{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddbd254",
   "metadata": {},
   "source": [
    "Aims:\n",
    "- Import flavin signals\n",
    "- Process them\n",
    "- Label them as oscillating, non-oscillating, or unsure if I haven't done it already\n",
    "- Train SVM and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6ccfd",
   "metadata": {},
   "source": [
    "Paradigms:\n",
    "- Leverage `pandas`: ease transition to `stoa`, easier to manipulate with `scikit-learn`, cleaner code\n",
    "- Discard unnecessary information (including births) & processes\n",
    "- Ultimate goal to put the parameters in each cell together in a `dict` and put the code in a pipeline, like everything in `stoa`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce8122",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE: USE THE `aliby` VIRTUAL ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams.update({'font.family': 'Noto Sans'})\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efb141",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# PARAMETERS\n",
    "#filename_prefix = './data/arin/Omero19979_'\n",
    "filename_prefix = './data/arin/Omero20016_'\n",
    "#\n",
    "\n",
    "# Import flavin signals\n",
    "signal = pd.read_csv(filename_prefix+'flavin.csv')\n",
    "signal.replace(0, np.nan, inplace=True) # because the CSV is constructed like that :/\n",
    "\n",
    "# Import look-up table for strains (would prefer to directly CSV -> dict)\n",
    "strainlookup_df = pd.read_csv(filename_prefix+'strains.csv')\n",
    "strainlookup_dict = dict(zip(strainlookup_df.position, strainlookup_df.strain))\n",
    "\n",
    "# Positions -> Strain (more informative)\n",
    "signal = signal.replace({'position': strainlookup_dict})\n",
    "signal.rename(columns = {\"position\": \"strain\"}, inplace = True)\n",
    "signal = signal.drop(['distfromcentre'], axis = 1)\n",
    "\n",
    "# Convert to multi-index dataframe\n",
    "signal_temp = signal.iloc[:,2:]\n",
    "multiindex = pd.MultiIndex.from_frame(signal[['strain', 'cellID']])\n",
    "signal = pd.DataFrame(signal_temp.to_numpy(),\n",
    "                      index = multiindex)\n",
    "\n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236e5d4",
   "metadata": {},
   "source": [
    "# Choose a list of cells as working data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ec373",
   "metadata": {},
   "source": [
    "List strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f019f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.index.get_level_values(0).unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01175b0",
   "metadata": {},
   "source": [
    "Define `signal_wd` as working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_wd = signal.loc[['by4741']]\n",
    "\n",
    "signal_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_wd = signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afe75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_processed.index.get_level_values(0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c430b",
   "metadata": {},
   "source": [
    "# Processing time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36540e8f",
   "metadata": {},
   "source": [
    "## Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efb37e",
   "metadata": {},
   "source": [
    "Chop up time series according to `interval_start` and `interval_end`, then remove cells that have NaNs.  Print number of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "interval_start = 25\n",
    "interval_end = 168\n",
    "#\n",
    "\n",
    "signal_processed = signal_wd.iloc[:, interval_start:interval_end].dropna()\n",
    "\n",
    "signal_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92ab37",
   "metadata": {},
   "source": [
    "## Detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71007a04",
   "metadata": {},
   "source": [
    "Using sliding window (Al√°n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from postprocessor.core.processes.detrend import detrendParameters, detrend\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(signal_processed)\n",
    "plt.title('Before detrending')\n",
    "plt.show()\n",
    "\n",
    "detrend_runner = detrend(detrendParameters.default())\n",
    "signal_norm = detrend_runner.run(signal_processed)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(signal_norm)\n",
    "plt.title('After detrending')\n",
    "plt.show()\n",
    "\n",
    "signal_processed = signal_norm\n",
    "\n",
    "signal_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37dc9a",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922f1db",
   "metadata": {},
   "source": [
    "Option 1: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c69b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b180893",
   "metadata": {},
   "source": [
    "# Assign labels (if not already done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab6bda",
   "metadata": {},
   "source": [
    "Assign labels by scoring oscillations (human), and save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7173597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# PARAMETERS\n",
    "filename_category = 'test.csv'\n",
    "#\n",
    "\n",
    "category_list = []\n",
    "for timeseries in signal_processed.to_numpy():\n",
    "    plt.plot(timeseries)\n",
    "    plt.show(block=False)\n",
    "    category = input('Is this oscillatory?: ')\n",
    "    category_list.append(category)\n",
    "category_df = pd.DataFrame(category_list, index = signal_processed.index)\n",
    "category_df.to_csv(filename_category, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782d08e",
   "metadata": {},
   "source": [
    "Or, randomise scores and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01578f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "filename_category = 'random.csv'\n",
    "#category_labels = [0,1,2]\n",
    "#weights = [51/294, 135/294, 108/294]\n",
    "category_labels = [0,1]\n",
    "weights = [345/678, 333/678]\n",
    "#\n",
    "\n",
    "category_df = pd.DataFrame(\n",
    "    [np.random.choice(category_labels, 1, p=weights) for i in range(len(signal_processed))],\n",
    "    index = signal_processed.index\n",
    ")\n",
    "category_df.to_csv(filename_category, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41f102",
   "metadata": {},
   "source": [
    "# Featurisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dde1de",
   "metadata": {},
   "source": [
    "TODO: Make choice of feature some kind of parameter within the overarching pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead78d",
   "metadata": {},
   "source": [
    "Option 1: Use `catch22`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebab930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.catch22 import catch22Parameters, catch22\n",
    "\n",
    "catch22_processor = catch22(catch22Parameters.default())\n",
    "features = catch22_processor.run(signal_processed)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087fbc1",
   "metadata": {},
   "source": [
    "Additionally, choose a subset of the `catch22` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset = [\n",
    "    'PD_PeriodicityWang_th0_01',\n",
    "    'FC_LocalSimple_mean1_tauresrat',\n",
    "    'SB_MotifThree_quantile_hh',\n",
    "    'CO_Embed2_Dist_tau_d_expfit_meandiff',\n",
    "    'SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1',\n",
    "    #'CO_FirstMin_ac',\n",
    "    #'CO_f1ecac',\n",
    "]\n",
    "features = features[features_subset]\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63b4b0",
   "metadata": {},
   "source": [
    "Option 2: FFT spectrum\n",
    "\n",
    "(Caution: there may be slight variations between this and the old notebook -- could be yet-to-be-debugged different behaviour in `postprocessor.core.processes.fft`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890524f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.fft import fftParameters, fft\n",
    "\n",
    "fft_processor = fft(fftParameters.default())\n",
    "_, features = fft_processor.run(signal_processed)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574b388",
   "metadata": {},
   "source": [
    "Option 3: concatenate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.catch22 import catch22Parameters, catch22\n",
    "from postprocessor.core.processes.fft import fftParameters, fft\n",
    "\n",
    "catch22_processor = catch22(catch22Parameters.default())\n",
    "catch22_features = catch22_processor.run(signal_processed)\n",
    "fft_processor = fft(fftParameters.default())\n",
    "_, fft_features = fft_processor.run(signal_processed)\n",
    "features = pd.concat([catch22_features, fft_features], axis=1)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb268e",
   "metadata": {},
   "source": [
    "# Classifier pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e1712",
   "metadata": {},
   "source": [
    "- Import targets (labels, e.g. oscillatory vs non-oscillatory)\n",
    "- Construct pipeline: detrend, featurise with `catch22`, classifier/model\n",
    "- Grid search and apply best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f102b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current intention: run this after import and chopping up time series.\n",
    "# The `signal_processed` variable should be defined at this point.\n",
    "# Refactoring (splitting each part into its own script) will come later...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "from postprocessor.core.processes.detrend import detrendParameters, detrend\n",
    "from postprocessor.core.processes.catch22 import catch22Parameters, catch22\n",
    "\n",
    "# PARAMETERS\n",
    "filename_targets = 'categories_20016_detrend.csv'\n",
    "#\n",
    "\n",
    "# (add import, chopping up time series)\n",
    "\n",
    "# Import target values\n",
    "targets_df = pd.read_csv(filename_targets, header = None, index_col = 0)\n",
    "targets_df.index.names = ['cellID']\n",
    "targets = targets_df.loc[signal_processed.index.get_level_values('cellID')].to_numpy().flatten()\n",
    "# Converts whatever the target values are to 0 and 1\n",
    "#targets = np.array([np.argwhere(np.unique(targets) == element).flatten()[0] for element in targets])\n",
    "\n",
    "## TODO: option to remove class 2 ones if three classes defined but I want a binary classifier\n",
    "\n",
    "# Wrap post-processes into objects that scikit-learn can make a pipeline from\n",
    "class DetrendTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window = 45):\n",
    "        self.window = window\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y = None):\n",
    "        detrend_params = detrendParameters(self.window)\n",
    "        detrend_runner = detrend(detrend_params)\n",
    "        return detrend_runner.run(x)\n",
    "    \n",
    "class Catch22Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        catch22_runner = catch22(catch22Parameters.default())\n",
    "        return catch22_runner.run(x)\n",
    "\n",
    "# Set up for hyperparameter grid search\n",
    "window_range = [45]\n",
    "#C_range = [1] #np.logspace(-3, 3, 3)\n",
    "#gamma_range = np.logspace(-3, 3, 3)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'detrend__window': window_range,\n",
    "        'classifier__kernel': ['rbf'],\n",
    "        #'classifier__C': C_range,\n",
    "#        'classifier__gamma': gamma_range\n",
    "    },\n",
    "]\n",
    "# Create pipeline, with classifier\n",
    "my_pipeline = Pipeline(\n",
    "    [\n",
    "        ('detrend', DetrendTransformer(window = 45)),\n",
    "        ('featurise', Catch22Transformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(probability=True)),\n",
    "        #RandomForestClassifier(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if False:\n",
    "    features = signal_processed\n",
    "    my_pipeline.fit(features, targets)\n",
    "\n",
    "# Grid search (takes a while...)\n",
    "grid_pipeline = GridSearchCV(my_pipeline, param_grid, cv = 5)\n",
    "features = signal_processed\n",
    "grid_pipeline.fit(features, targets)\n",
    "\n",
    "my_pipeline.set_params(**grid_pipeline.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f834",
   "metadata": {},
   "source": [
    "- Training, evaluate using precision & recall\n",
    "- Histogram of probabilities (if SVM)\n",
    "- k-fold cross-validation\n",
    "- ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "train_size = 0.75# * len(features)\n",
    "#\n",
    "\n",
    "# Split training & testing\n",
    "if True:\n",
    "    features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "        features, targets,\n",
    "        train_size = train_size,\n",
    "    )\n",
    "\n",
    "## TODO: implement expanded testing list in the case I want a binary classifier but the data was classified into three classes.\n",
    "\n",
    "if True:\n",
    "    # Fit\n",
    "    my_pipeline.fit(features_train, targets_train)\n",
    "\n",
    "    # Predict categories\n",
    "    targets_predicted = my_pipeline.predict(features_test)\n",
    "    # Print cellIDs predicted to be in each category\n",
    "    predictions_dict = {}\n",
    "    for class_label in set(targets):\n",
    "        predictions_dict[class_label] = features_test.iloc[targets_predicted == class_label].index.to_numpy()\n",
    "    print('Predictions')\n",
    "    print(predictions_dict)\n",
    "\n",
    "    # Get probabilities\n",
    "    targets_proba = my_pipeline.predict_proba(features_test)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    targets_proba_df = pd.DataFrame(targets_proba, index = features_test.index)\n",
    "    targets_proba_df.sort_values(by=[1]) # sorted by probability of oscillation\n",
    "    # Plot histogram of probabilities\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(targets_proba_df.iloc[:,1], 40,\n",
    "            color = '#3714b0')\n",
    "    plt.title('Histogram of probabilities')\n",
    "    plt.xlabel('Probability of oscillation')\n",
    "    plt.ylabel('Number of time series')\n",
    "\n",
    "    # Verify by doing it again with k-fold cross-validation\n",
    "    n_splits = 5\n",
    "    kf = StratifiedKFold(n_splits = n_splits)\n",
    "    print('k-fold cross-validation')\n",
    "    kf_scores = []\n",
    "    for train_index, test_index in kf.split(features, targets):\n",
    "        # Split training-testing\n",
    "        features_train_kf, features_test_kf = features.iloc[train_index], features.iloc[test_index]\n",
    "        targets_train_kf, targets_test_kf = targets[train_index], targets[test_index]\n",
    "\n",
    "        # Train & predict\n",
    "        my_pipeline.fit(features_train_kf, targets_train_kf)\n",
    "        targets_predicted_kf = my_pipeline.predict(features_test_kf)\n",
    "\n",
    "        kf_precision = precision_score(targets_test_kf, targets_predicted_kf, average='weighted')\n",
    "        kf_recall = recall_score(targets_test_kf, targets_predicted_kf, average='weighted')\n",
    "        \n",
    "        # Compute measures\n",
    "        print(\n",
    "            'Precision ' +\n",
    "            '%.4f' % kf_precision +\n",
    "            ' Recall ' +\n",
    "            '%.4f' % kf_recall\n",
    "            )\n",
    "        \n",
    "        kf_scores.append([kf_precision, kf_recall])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for split in list(range(n_splits)):\n",
    "        x_pos = np.arange(2)\n",
    "        x_pos = [pos + split/8 for pos in x_pos]\n",
    "        plt.bar(\n",
    "            x_pos,\n",
    "            kf_scores[split],\n",
    "            width = 0.125\n",
    "         )\n",
    "    plt.xticks([0.25, 1.25], ['Precision', 'Recall'])\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(str(n_splits)+'-fold cross-validation')\n",
    "        \n",
    "    # ROC curve\n",
    "    scores = targets_proba_df.iloc[:,1]\n",
    "    false_positive_rate, true_positive_rate, _ = roc_curve(targets_test, scores)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(false_positive_rate, true_positive_rate)\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    print('ROC curve: area under curve is ' + '%.4f' % auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31530da4",
   "metadata": {},
   "source": [
    "If using a random forest classifier, get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    classifier['randomforestclassifier'].feature_importances_,\n",
    "    index = features.columns,\n",
    "    columns = ['importance'],\n",
    ")\n",
    "feature_importances_sorted = feature_importances.sort_values(\n",
    "    ['importance'],\n",
    "    ascending = False,\n",
    ")\n",
    "feature_importances_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1052e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "plt.plot(np.cumsum(feature_importances_sorted.to_numpy()))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cumulative importance')\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.bar(\n",
    "    x = np.arange(len(feature_importances)),\n",
    "    height = feature_importances.to_numpy().T[0],\n",
    "    tick_label = feature_importances.index,\n",
    ")\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2b2b",
   "metadata": {},
   "source": [
    "TODO: Test block for training on one dataset and testing on another"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aliby",
   "language": "python",
   "name": "aliby"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
