{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ab6f52",
   "metadata": {},
   "source": [
    "Aims:\n",
    "- Import flavin signals\n",
    "- Process them\n",
    "- Label them as oscillating, non-oscillating, or unsure if I haven't done it already\n",
    "- Train SVM and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18261ab6",
   "metadata": {},
   "source": [
    "Paradigms:\n",
    "- Leverage `pandas`: ease transition to `stoa`, easier to manipulate with `scikit-learn`, cleaner code\n",
    "- Discard unnecessary information (including births) & processes\n",
    "- Ultimate goal to put the parameters in each cell together in a `dict` and put the code in a pipeline, like everything in `stoa`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6b5d9",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE: USE THE `stoa` VIRTUAL ENVIRONMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e83d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a479f",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# PARAMETERS\n",
    "filename_prefix = './data/arin/Omero19979_'\n",
    "#filename_prefix = './data/arin/Omero20016_'\n",
    "#\n",
    "\n",
    "# Import flavin signals\n",
    "signal = pd.read_csv(filename_prefix+'flavin.csv')\n",
    "signal.replace(0, np.nan, inplace=True) # because the CSV is constructed like that :/\n",
    "\n",
    "# Import look-up table for strains (would prefer to directly CSV -> dict)\n",
    "strainlookup_df = pd.read_csv(filename_prefix+'strains.csv')\n",
    "strainlookup_dict = dict(zip(strainlookup_df.position, strainlookup_df.strain))\n",
    "\n",
    "# Positions -> Strain (more informative)\n",
    "signal = signal.replace({'position': strainlookup_dict})\n",
    "signal.rename(columns = {\"position\": \"strain\"}, inplace = True)\n",
    "signal = signal.drop(['distfromcentre'], axis = 1)\n",
    "\n",
    "# Convert to multi-index dataframe\n",
    "signal_temp = signal.iloc[:,2:]\n",
    "multiindex = pd.MultiIndex.from_frame(signal[['strain', 'cellID']])\n",
    "signal = pd.DataFrame(signal_temp.to_numpy(),\n",
    "                      index = multiindex)\n",
    "\n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5252543",
   "metadata": {},
   "source": [
    "# Choose a list of cells as working data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd369b7",
   "metadata": {},
   "source": [
    "List strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.index.get_level_values(0).unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2e93a",
   "metadata": {},
   "source": [
    "Define `signal_wd` as working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c954b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_wd = signal.loc['rim11_Del']\n",
    "\n",
    "signal_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80599b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_wd = signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08da2ca",
   "metadata": {},
   "source": [
    "# Processing time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7caa769",
   "metadata": {},
   "source": [
    "## Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc9261",
   "metadata": {},
   "source": [
    "Chop up time series according to `interval_start` and `interval_end`, then remove cells that have NaNs.  Print number of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "interval_start = 25\n",
    "interval_end = 168\n",
    "#\n",
    "\n",
    "signal_processed = signal_wd.iloc[:, interval_start:interval_end].dropna()\n",
    "\n",
    "signal_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399c9d7",
   "metadata": {},
   "source": [
    "## Detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c3476",
   "metadata": {},
   "source": [
    "Using sliding window (Al√°n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PARAMETERS\n",
    "window = 45\n",
    "#\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(signal_processed)\n",
    "plt.title('Before detrending')\n",
    "plt.show()\n",
    "\n",
    "def moving_average(input_timeseries,\n",
    "                  window = 3):\n",
    "    processed_timeseries = np.cumsum(input_timeseries, dtype=float)\n",
    "    processed_timeseries[window:] = processed_timeseries[window:] - processed_timeseries[:-window]\n",
    "    return processed_timeseries[window - 1 :] /  window\n",
    "\n",
    "signal_processed = signal_processed.div(signal_processed.mean(axis = 1), axis = 0)\n",
    "signal_movavg = signal_processed.apply(lambda x: pd.Series(moving_average(x.values, window)), axis = 1)\n",
    "signal_norm = signal_processed.iloc(axis = 1)[window//2: -window//2] / signal_movavg.iloc[:,0:signal_movavg.shape[1]-1].values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(signal_norm)\n",
    "plt.title('After detrending')\n",
    "plt.show()\n",
    "\n",
    "signal_processed = signal_norm\n",
    "\n",
    "signal_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6900e",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4097fd",
   "metadata": {},
   "source": [
    "Option 1: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cd623",
   "metadata": {},
   "source": [
    "# Assign labels (if not already done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689aab1c",
   "metadata": {},
   "source": [
    "Assign labels by scoring oscillations (human), and save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# PARAMETERS\n",
    "filename_category = 'test.csv'\n",
    "#\n",
    "\n",
    "category_list = []\n",
    "for timeseries in signal_processed.to_numpy():\n",
    "    plt.plot(timeseries)\n",
    "    plt.show(block=False)\n",
    "    category = input('Is this oscillatory?: ')\n",
    "    category_list.append(category)\n",
    "category_df = pd.DataFrame(category_list, index = signal_processed.index)\n",
    "category_df.to_csv(filename_category, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3f6d2",
   "metadata": {},
   "source": [
    "Or, randomise scores and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "filename_category = 'random.csv'\n",
    "#category_labels = [0,1,2]\n",
    "#weights = [51/294, 135/294, 108/294]\n",
    "category_labels = [0,1]\n",
    "weights = [345/678, 333/678]\n",
    "#\n",
    "\n",
    "category_df = pd.DataFrame(\n",
    "    [np.random.choice(category_labels, 1, p=weights) for i in range(len(signal_processed))],\n",
    "    index = signal_processed.index\n",
    ")\n",
    "category_df.to_csv(filename_category, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c7d3",
   "metadata": {},
   "source": [
    "# Featurisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed12c7b",
   "metadata": {},
   "source": [
    "TODO: Make choice of feature some kind of parameter within the overarching pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232979f2",
   "metadata": {},
   "source": [
    "Option 1: Use `catch22`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97222be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.catch22 import catch22Parameters, catch22\n",
    "\n",
    "catch22_processor = catch22(catch22Parameters.default())\n",
    "features = catch22_processor.run(signal_processed)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5336661",
   "metadata": {},
   "source": [
    "Additionally, choose a subset of the `catch22` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset = [\n",
    "    'CO_Embed2_Dist_tau_d_expfit_meandiff',\n",
    "    'SP_Summaries_welch_rect_area_5_1',\n",
    "    'SB_MotifThree_quantile_hh',\n",
    "    'FC_LocalSimple_mean1_tauresrat',\n",
    "    #'CO_f1ecac',\n",
    "]\n",
    "features = features[features_subset]\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0925ec",
   "metadata": {},
   "source": [
    "Option 2: FFT spectrum\n",
    "\n",
    "(Caution: there may be slight variations between this and the old notebook -- could be yet-to-be-debugged different behaviour in `postprocessor.core.processes.fft`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15808b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.fft import fftParameters, fft\n",
    "\n",
    "fft_processor = fft(fftParameters.default())\n",
    "_, features = fft_processor.run(signal_processed)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056c807",
   "metadata": {},
   "source": [
    "Option 3: concatenate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessor.core.processes.catch22 import catch22Parameters, catch22\n",
    "from postprocessor.core.processes.fft import fftParameters, fft\n",
    "\n",
    "catch22_processor = catch22(catch22Parameters.default())\n",
    "catch22_features = catch22_processor.run(signal_processed)\n",
    "fft_processor = fft(fftParameters.default())\n",
    "_, fft_features = fft_processor.run(signal_processed)\n",
    "features = pd.concat([catch22_features, fft_features], axis=1)\n",
    "\n",
    "sns.heatmap(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f2fd9",
   "metadata": {},
   "source": [
    "# Classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2730a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# PARAMETERS\n",
    "filename_targets = 'categories_19979_detrend.csv'\n",
    "#filename_targets = 'random.csv'\n",
    "train_size = 150\n",
    "#\n",
    "\n",
    "# (add import, processing)\n",
    "\n",
    "# (add featurisation)\n",
    "\n",
    "# Import target values\n",
    "targets_df = pd.read_csv(filename_targets, header = None, index_col = 0)\n",
    "targets_df.index.names = ['cellID']\n",
    "targets = targets_df.loc[features.index.get_level_values('cellID')].to_numpy().flatten()\n",
    "\n",
    "## TODO: option to remove class 2 ones if three classes defined but I want a binary classifier\n",
    "\n",
    "# Create classifier (pipeline)\n",
    "classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    #SVC(gamma='auto', probability=True),\n",
    "    RandomForestClassifier(),\n",
    ")\n",
    "\n",
    "# Split training & testing\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets,\n",
    "    train_size = train_size,\n",
    ")\n",
    "\n",
    "## TODO: implement expanded testing list in the case I want a binary classifier but the data was classified into three classes.\n",
    "\n",
    "# Fit\n",
    "classifier.fit(features_train, targets_train)\n",
    "\n",
    "# Predict categories\n",
    "targets_predicted = classifier.predict(features_test)\n",
    "# Print cellIDs predicted to be in each category\n",
    "predictions_dict = {}\n",
    "for class_label in set(targets):\n",
    "    predictions_dict[class_label] = features_test.iloc[targets_predicted == class_label].index.to_numpy()\n",
    "print('Predictions')\n",
    "print(predictions_dict)\n",
    "\n",
    "# Get probabilities\n",
    "targets_proba = classifier.predict_proba(features_test)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "targets_proba_df = pd.DataFrame(targets_proba, index = features_test.index)\n",
    "targets_proba_df.sort_values(by=[1]) # sorted by probability of oscillation\n",
    "# Plot histogram of probabilities\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(targets_proba_df.iloc[:,1], 40)\n",
    "plt.title('Histogram of probabilities')\n",
    "plt.xlabel('Probability of oscillation')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "## TODO: visualisations of time series & feature vectors of each group\n",
    "\n",
    "# Verify by doing it again with k-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits = 5)\n",
    "print('k-fold cross-validation')\n",
    "for train_index, test_index in kf.split(features, targets):\n",
    "    # Split training-testing\n",
    "    features_train_kf, features_test_kf = features.iloc[train_index], features.iloc[test_index]\n",
    "    targets_train_kf, targets_test_kf = targets[train_index], targets[test_index]\n",
    "    \n",
    "    # Train & predict\n",
    "    classifier.fit(features_train_kf, targets_train_kf)\n",
    "    targets_predicted_kf = classifier.predict(features_test_kf)\n",
    "    \n",
    "    # Compute measures\n",
    "    print(\n",
    "        'Precision ' +\n",
    "        '%.4f' % precision_score(targets_test_kf, targets_predicted_kf, average='weighted') +\n",
    "        ' Recall ' +\n",
    "        '%.4f' % recall_score(targets_test_kf, targets_predicted_kf, average='weighted')\n",
    "        )\n",
    "\n",
    "# ROC curve\n",
    "scores = targets_proba_df.iloc[:,1]\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(targets_test, scores)\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "print('ROC curve: area under curve is ' + '%.4f' % auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651be263",
   "metadata": {},
   "source": [
    "If using a random forest classifier, get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    classifier['randomforestclassifier'].feature_importances_,\n",
    "    index = features.columns,\n",
    "    columns = ['importance'],\n",
    ")\n",
    "feature_importances_sorted = feature_importances.sort_values(\n",
    "    ['importance'],\n",
    "    ascending = False,\n",
    ")\n",
    "feature_importances_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ac0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "plt.plot(np.cumsum(feature_importances_sorted.to_numpy()))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cumulative importance')\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.bar(\n",
    "    x = np.arange(len(feature_importances)),\n",
    "    height = feature_importances.to_numpy().T[0],\n",
    "    tick_label = feature_importances.index,\n",
    ")\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99f712",
   "metadata": {},
   "source": [
    "Test block for training on one dataset and testing on another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "filename_targets = 'categories_zwf_detrend.csv'\n",
    "#filename_targets = 'random.csv'\n",
    "train_size = 150\n",
    "#\n",
    "\n",
    "# (add import, processing)\n",
    "\n",
    "# (add featurisation)\n",
    "\n",
    "# Import target values\n",
    "targets = pd.read_csv(filename_targets, header = None, index_col = 0)\n",
    "targets.index.names = ['cellID']\n",
    "targets = targets.to_numpy().flatten()\n",
    "\n",
    "## TODO: option to remove class 2 ones if three classes defined but I want a binary classifier\n",
    "\n",
    "# Split training & testing\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets,\n",
    "    train_size = train_size,\n",
    ")\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "# Predict categories\n",
    "targets_predicted = classifier.predict(features_test)\n",
    "# Print cellIDs predicted to be in each category\n",
    "predictions_dict = {}\n",
    "for class_label in set(targets):\n",
    "    predictions_dict[class_label] = features_test.iloc[targets_predicted == class_label].index.to_numpy()\n",
    "print('Predictions')\n",
    "print(predictions_dict)\n",
    "\n",
    "# Get probabilities\n",
    "targets_proba = classifier.predict_proba(features_test)\n",
    "pd.set_option('display.max_rows', None)\n",
    "targets_proba_df = pd.DataFrame(targets_proba, index = features_test.index)\n",
    "targets_proba_df.sort_values(by=[1]) # sorted by probability of oscillation\n",
    "# Plot histogram of probabilities\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(targets_proba_df.iloc[:,1], 40)\n",
    "plt.title('Histogram of probabilities')\n",
    "plt.xlabel('Probability of oscillation')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "## TODO: visualisations of time series & feature vectors of each group\n",
    "\n",
    "# Verify by doing it again with k-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits = 5)\n",
    "print('k-fold cross-validation')\n",
    "for train_index, test_index in kf.split(features, targets):\n",
    "    # Split training-testing\n",
    "    features_train_kf, features_test_kf = features.iloc[train_index], features.iloc[test_index]\n",
    "    targets_train_kf, targets_test_kf = targets[train_index], targets[test_index]\n",
    "    \n",
    "    # Train & predict\n",
    "    classifier.fit(features_train_kf, targets_train_kf)\n",
    "    targets_predicted_kf = classifier.predict(features_test_kf)\n",
    "    \n",
    "    # Compute measures\n",
    "    print(\n",
    "        'Precision ' +\n",
    "        '%.4f' % precision_score(targets_test_kf, targets_predicted_kf, average='weighted') +\n",
    "        ' Recall ' +\n",
    "        '%.4f' % recall_score(targets_test_kf, targets_predicted_kf, average='weighted')\n",
    "        )\n",
    "\n",
    "# ROC curve\n",
    "scores = targets_proba_df.iloc[:,1]\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(targets_test, scores)\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "print('ROC curve: area under curve is ' + '%.4f' % auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cef2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do something with this thing\n",
    "\n",
    "Wlist_backup_MATLABids = [cell.MATLABid for cell in Wlist_backup]\n",
    "Traininglist_MATLABids = [cell.MATLABid for cell in Traininglist]\n",
    "\n",
    "Testinglist_expanded = [cell for cell in Wlist_backup if cell.MATLABid not in Traininglist_MATLABids]\n",
    "\n",
    "testing_data_expanded = np.array([cell.flavin.feature_vector for cell in Testinglist_expanded])\n",
    "\n",
    "len(set(Testinglist_expanded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aliby",
   "language": "python",
   "name": "aliby"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
